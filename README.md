# Hi there 👋, I'm Christopher Wachira!

I'm a **Data Engineer** and **Data Scientist** passionate about transforming raw data into actionable insights. With extensive experience in data pipelines, ETL processes, data validation, and analytics, I’m dedicated to helping organizations make data-driven decisions efficiently. 

---

### 👨‍💻 About Me
- 💼 Currently working on building scalable and automated data solutions.
- 🌐 Hands-on experience with cloud platforms like **AWS** and data management with **Redshift** and **MySQL**.
- 📊 Skilled in data warehousing, migration, data lake architectures, and cross-platform data integration.
- 💡 Always eager to explore new tools and best practices in data engineering and data science.

---

### 🛠️ Technical Skills
- **Programming:** Python, SQL
- **Data Warehousing & Databases:** Redshift, MySQL, PostgreSQL, SAP (for inventory data)
- **Cloud Platforms:** AWS (S3, DMS, Redshift)
- **Data Integration & ETL:** AWS DMS, Airflow (Dockerized for pipeline automation)
- **Data Analytics:** Pandas, NumPy, and SQL-based data validation
- **Visualization:** Power BI, Tableau (for dashboards and reports)
- **Others:** Docker, NGINX (for reverse proxy setups)

---

### 🏆 Projects & Experience Highlights
1. **Data Migration and ETL Pipelines with AWS DMS and Redshift**
   - Built and managed data pipelines to migrate CDC and Full Load data tasks from MySQL to Redshift.
   - Automated validations and alerts to monitor migration accuracy, using custom Python scripts for DMS Table Statistics analysis.

2. **Data Lake & Redshift Integration**
   - Developed scripts for consistent data fetch and insert processes from **S3** to **Redshift**, including automated data cleanup and deduplication for daily ingestion.
   - Designed workflow to handle data overflows, resolving issues like numeric data overflow in Redshift with precision handling.

3. **Inventory Transfer Automation with SAP**
   - Built an automation solution in SAP to monitor and notify discrepancies between goods dispatches and inventory transfer requests, ensuring data consistency and operational alignment.

4. **Open Data Discovery Platform**
   - Set up **Open Data Discovery** platform using Docker Compose, redshift, and google sso for datalake data discovery.

---

### 🔧 Tools & Technologies in My Workflow
- **Automation & Scheduling:** Apache Airflow, Python-based scripts, Cron jobs
- **CI/CD & Infrastructure Management:** Docker Compose, NGINX (for web and data platforms)
- **Notification & Alerts:** SNS, lambda, python, cloudwatch, Slack and Email integrations for proactive monitoring

---

### 🌱 I’m currently exploring:
- Enhancing my expertise in **Machine Learning** for predictive analytics.
- Optimizing data pipelines for faster ETL processes.
- Upskilling on data engineering, both on cloud and self managed.

---

### 📫 Let’s Connect
- **Email:** [chriswachira873@gmail.com](mailto:christopher.wachira@cellulant.io)
- **LinkedIn:** [linkedin.com/in/christopherwachira](https://www.linkedin.com/in/christopherwachira)

Feel free to check out my repositories for some of my work on data engineering solutions, automation, and analytics!
